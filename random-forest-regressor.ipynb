{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9669606,"sourceType":"datasetVersion","datasetId":5908844}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Random Forest Regressor\n\n**Random Forest** is an ensemble learning method that combines multiple decision trees to improve predictive accuracy and control overfitting. It operates on the principle of \"bagging\" (Bootstrap Aggregating) and introduces randomness in the selection of features to enhance the diversity of the individual trees.\n\n## How Random Forest Works\n\n### 1. Bootstrapping\n\nThe first step in constructing a random forest involves generating multiple subsets of the training dataset using the bootstrapping technique:\n- Each tree in the forest is trained on a different random sample of the dataset, created by sampling with replacement.\n- If the original dataset contains N samples, each tree is trained on n samples, where typically n ≈ N.\n\n### 2. Building Decision Trees\n\nFor each bootstrapped sample, a decision tree is constructed:\n- **Node Splitting**: At each node of the tree, a subset of features is randomly selected. Instead of considering all features for splitting, a smaller number m of features are randomly chosen.\n  \n  - For classification:\n    - m = sqrt(M)\n  \n  - For regression:\n    - m = M / 3\n\n  where M is the total number of features.\n\n- **Impurity Calculation**: For regression, the impurity is often measured using the Mean Squared Error (MSE):\n  \n  - MSE(t) = (1 / N_t) * Σ (y_i - ȳ_t)²\n\n  where:\n  - N_t is the number of observations in the node t.\n  - y_i is the actual value of the i-th observation.\n  - ȳ_t is the mean of the target values in the node t.\n\n### 3. Tree Growth\n\nEach decision tree is grown to its maximum depth without pruning:\n- Nodes are split until the stopping criteria are met, which may include:\n  - A maximum depth is reached.\n  - A minimum number of samples in a leaf node (e.g., `min_samples_leaf`).\n  - A minimum number of samples required to split an internal node (e.g., `min_samples_split`).\n\n### 4. Prediction\n\nOnce all trees in the forest are constructed, predictions are made:\n- **Regression**: The predicted value is obtained by averaging the predictions from all trees.\n\n  - ŷ = (1 / T) * Σ y_t\n\n  where:\n  - ŷ is the predicted value.\n  - y_t is the prediction from tree t.\n  - T is the total number of trees.\n\n### 5. Feature Importance\n\nRandom Forest can also provide insights into feature importance:\n- The importance of each feature can be determined by measuring how much the model's accuracy decreases when the values of that feature are permuted.\n- Another common method is to evaluate the average decrease in impurity (e.g., MSE) brought by each feature across all trees.\n\n## Hyperparameters\n\nKey hyperparameters in Random Forest include:\n\n- **n_estimators**: The number of trees in the forest.\n- **max_depth**: The maximum depth of each tree.\n- **min_samples_split**: The minimum number of samples required to split an internal node.\n- **min_samples_leaf**: The minimum number of samples required to be at a leaf node.\n- **max_features**: The number of features to consider when looking for the best split. This can be:\n  - A fraction of the total number of features (e.g., `sqrt` or `log2`).\n  - A specific number of features.\n\n## Advantages\n\n- **Reduced Overfitting**: Random Forest helps mitigate overfitting by averaging the predictions of multiple trees.\n- **Robustness**: It is less sensitive to noise in the data compared to a single decision tree.\n- **Feature Importance**: Provides insights into which features are most important for predictions.\n\n## Disadvantages\n\n- **Complexity**: Random Forest is less interpretable than a single decision tree.\n- **Computationally Intensive**: Training multiple trees requires more computational resources and time.\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-10-19T18:43:30.190596Z","iopub.execute_input":"2024-10-19T18:43:30.191076Z","iopub.status.idle":"2024-10-19T18:43:31.665351Z","shell.execute_reply.started":"2024-10-19T18:43:30.191015Z","shell.execute_reply":"2024-10-19T18:43:31.664028Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/housing3/BostonHousing.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T18:43:42.828743Z","iopub.execute_input":"2024-10-19T18:43:42.829291Z","iopub.status.idle":"2024-10-19T18:43:42.856338Z","shell.execute_reply.started":"2024-10-19T18:43:42.829241Z","shell.execute_reply":"2024-10-19T18:43:42.855183Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-10-19T18:43:45.308832Z","iopub.execute_input":"2024-10-19T18:43:45.309304Z","iopub.status.idle":"2024-10-19T18:43:45.362743Z","shell.execute_reply.started":"2024-10-19T18:43:45.309260Z","shell.execute_reply":"2024-10-19T18:43:45.361371Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"        crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n0    0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n1    0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242   \n3    0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n4    0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n..       ...   ...    ...   ...    ...    ...   ...     ...  ...  ...   \n501  0.06263   0.0  11.93     0  0.573  6.593  69.1  2.4786    1  273   \n502  0.04527   0.0  11.93     0  0.573  6.120  76.7  2.2875    1  273   \n503  0.06076   0.0  11.93     0  0.573  6.976  91.0  2.1675    1  273   \n504  0.10959   0.0  11.93     0  0.573  6.794  89.3  2.3889    1  273   \n505  0.04741   0.0  11.93     0  0.573  6.030  80.8  2.5050    1  273   \n\n     ptratio       b  lstat  medv  \n0       15.3  396.90   4.98  24.0  \n1       17.8  396.90   9.14  21.6  \n2       17.8  392.83   4.03  34.7  \n3       18.7  394.63   2.94  33.4  \n4       18.7  396.90   5.33  36.2  \n..       ...     ...    ...   ...  \n501     21.0  391.99   9.67  22.4  \n502     21.0  396.90   9.08  20.6  \n503     21.0  396.90   5.64  23.9  \n504     21.0  393.45   6.48  22.0  \n505     21.0  396.90   7.88  11.9  \n\n[506 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>crim</th>\n      <th>zn</th>\n      <th>indus</th>\n      <th>chas</th>\n      <th>nox</th>\n      <th>rm</th>\n      <th>age</th>\n      <th>dis</th>\n      <th>rad</th>\n      <th>tax</th>\n      <th>ptratio</th>\n      <th>b</th>\n      <th>lstat</th>\n      <th>medv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1</td>\n      <td>296</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.06263</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0</td>\n      <td>0.573</td>\n      <td>6.593</td>\n      <td>69.1</td>\n      <td>2.4786</td>\n      <td>1</td>\n      <td>273</td>\n      <td>21.0</td>\n      <td>391.99</td>\n      <td>9.67</td>\n      <td>22.4</td>\n    </tr>\n    <tr>\n      <th>502</th>\n      <td>0.04527</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0</td>\n      <td>0.573</td>\n      <td>6.120</td>\n      <td>76.7</td>\n      <td>2.2875</td>\n      <td>1</td>\n      <td>273</td>\n      <td>21.0</td>\n      <td>396.90</td>\n      <td>9.08</td>\n      <td>20.6</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>0.06076</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0</td>\n      <td>0.573</td>\n      <td>6.976</td>\n      <td>91.0</td>\n      <td>2.1675</td>\n      <td>1</td>\n      <td>273</td>\n      <td>21.0</td>\n      <td>396.90</td>\n      <td>5.64</td>\n      <td>23.9</td>\n    </tr>\n    <tr>\n      <th>504</th>\n      <td>0.10959</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0</td>\n      <td>0.573</td>\n      <td>6.794</td>\n      <td>89.3</td>\n      <td>2.3889</td>\n      <td>1</td>\n      <td>273</td>\n      <td>21.0</td>\n      <td>393.45</td>\n      <td>6.48</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>505</th>\n      <td>0.04741</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0</td>\n      <td>0.573</td>\n      <td>6.030</td>\n      <td>80.8</td>\n      <td>2.5050</td>\n      <td>1</td>\n      <td>273</td>\n      <td>21.0</td>\n      <td>396.90</td>\n      <td>7.88</td>\n      <td>11.9</td>\n    </tr>\n  </tbody>\n</table>\n<p>506 rows × 14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"target = data[\"medv\"]\nx = data.drop(columns = [\"medv\"])","metadata":{"execution":{"iopub.status.busy":"2024-10-19T18:46:35.634051Z","iopub.execute_input":"2024-10-19T18:46:35.634531Z","iopub.status.idle":"2024-10-19T18:46:35.643919Z","shell.execute_reply.started":"2024-10-19T18:46:35.634459Z","shell.execute_reply":"2024-10-19T18:46:35.642641Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\nx_train,x_test,y_train,y_test = train_test_split(x,target,test_size=0.2,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T18:46:38.189462Z","iopub.execute_input":"2024-10-19T18:46:38.190791Z","iopub.status.idle":"2024-10-19T18:46:38.200644Z","shell.execute_reply.started":"2024-10-19T18:46:38.190736Z","shell.execute_reply":"2024-10-19T18:46:38.199400Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_regressor.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T18:49:46.609716Z","iopub.execute_input":"2024-10-19T18:49:46.610167Z","iopub.status.idle":"2024-10-19T18:49:47.056241Z","shell.execute_reply.started":"2024-10-19T18:49:46.610122Z","shell.execute_reply":"2024-10-19T18:49:47.055095Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"RandomForestRegressor(random_state=42)","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=42)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"y_pred = rf_regressor.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T18:49:48.791961Z","iopub.execute_input":"2024-10-19T18:49:48.792448Z","iopub.status.idle":"2024-10-19T18:49:48.810540Z","shell.execute_reply.started":"2024-10-19T18:49:48.792401Z","shell.execute_reply":"2024-10-19T18:49:48.809303Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T18:49:52.292388Z","iopub.execute_input":"2024-10-19T18:49:52.292867Z","iopub.status.idle":"2024-10-19T18:49:52.300532Z","shell.execute_reply.started":"2024-10-19T18:49:52.292823Z","shell.execute_reply":"2024-10-19T18:49:52.299257Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Mean Squared Error: 20.364713921568622\n","output_type":"stream"}]}]}